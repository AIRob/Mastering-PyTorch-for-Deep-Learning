{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mastering PyTorch\n",
    "\n",
    "## Supervised learning\n",
    "\n",
    "### Visualize the training in Visdom\n",
    "\n",
    "#### Accompanying notebook to Video 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include libraries\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from utils import get_image_name, get_number_of_cells, \\\n",
    "     split_data, download_data, SEED\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = './'\n",
    "download_data(root=root)\n",
    "\n",
    "data_paths = os.path.join('./', 'data_paths.txt')\n",
    "if not os.path.exists(data_paths):\n",
    "  !wget http://pbialecki.de/mastering_pytorch/data_paths.txt\n",
    "\n",
    "if not os.path.isfile(data_paths):\n",
    "    print('data_paths.txt missing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "# Setup Globals\n",
    "use_cuda = torch.cuda.is_available()\n",
    "data_paths = os.path.join('./', 'data', 'data_paths.txt')\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    print('Using: {}'.format(torch.cuda.get_device_name(0)))\n",
    "print_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def weights_init(m):\n",
    "    '''\n",
    "    Initialize the weights of each Conv2d layer using xavier_uniform\n",
    "    (\"Understanding the difficulty of training deep feedforward\n",
    "    neural networks\" - Glorot, X. & Bengio, Y. (2010))\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform(m.weight.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.xavier_uniform(m.weight.data)\n",
    "\n",
    "def dice_loss(y_target, y_pred, smooth=0.0):\n",
    "    y_target = y_target.view(-1)\n",
    "    y_pred = y_pred.view(-1)\n",
    "    intersection = (y_target * y_pred).sum()\n",
    "    dice_coef = (2. * intersection + smooth) / (\n",
    "        y_target.sum() + y_pred.sum() + smooth)\n",
    "    return 1. - dice_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CellDataset(Dataset):\n",
    "    def __init__(self, image_paths, target_paths, size, train=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.size = size\n",
    "        resize_size = [s+10 for s in self.size]\n",
    "        self.resize_image = transforms.Resize(\n",
    "            size=resize_size, interpolation=Image.BILINEAR)\n",
    "        self.resize_mask = transforms.Resize(\n",
    "            size=resize_size, interpolation=Image.NEAREST)\n",
    "        self.train = train\n",
    "        \n",
    "    def transform(self, image, mask):\n",
    "        # Resize\n",
    "        image = self.resize_image(image)\n",
    "        mask = self.resize_mask(mask)\n",
    "        \n",
    "        # Perform data augmentation\n",
    "        if self.train:            \n",
    "            # Random cropping\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(\n",
    "                image, output_size=self.size)\n",
    "            image = TF.crop(image, i, j, h, w)\n",
    "            mask = TF.crop(mask, i, j, h, w)\n",
    "            \n",
    "            # Random horizontal flipping\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.hflip(image)\n",
    "                mask = TF.hflip(mask)\n",
    "            \n",
    "            # Random vertical flipping\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.vflip(image)\n",
    "                mask = TF.vflip(mask)\n",
    "        else:\n",
    "            center_crop = transforms.CenterCrop(self.size)\n",
    "            image = center_crop(image)\n",
    "            mask = center_crop(mask)\n",
    "        \n",
    "        # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        mask = Image.open(self.target_paths[index])\n",
    "        x, y = self.transform(image, mask)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_sample(dataset):\n",
    "    '''\n",
    "    Get a random sample from the specified dataset.\n",
    "    '''\n",
    "    data, target = dataset[int(np.random.choice(len(dataset), 1))]\n",
    "    data.unsqueeze_(0)\n",
    "    target.unsqueeze_(0)\n",
    "    if use_cuda:\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "    data = Variable(data)\n",
    "    target = Variable(target)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding,\n",
    "                 stride):\n",
    "        super(BaseConv, self).__init__()\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding,\n",
    "                               stride)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size,\n",
    "                               padding, stride)\n",
    "        \n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size, padding, stride)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.act(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.act(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DownConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding,\n",
    "                 stride):\n",
    "        super(DownConv, self).__init__()\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv_block = BaseConv(in_channels, out_channels, kernel_size,\n",
    "                                   padding, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_channels, in_channels_skip, out_channels,\n",
    "                 kernel_size, padding, stride):\n",
    "        super(UpConv, self).__init__()\n",
    "\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(\n",
    "            in_channels, in_channels, kernel_size=2, padding=0, stride=2)\n",
    "        self.conv_block = BaseConv(\n",
    "            in_channels=in_channels + in_channels_skip,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            stride=stride)\n",
    "\n",
    "    def forward(self, x, x_skip):\n",
    "        x = self.conv_trans1(x)\n",
    "        x = torch.cat((x, x_skip), dim=1)\n",
    "        x = self.conv_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding,\n",
    "                 stride):\n",
    "        super(ResUNet, self).__init__()\n",
    "\n",
    "        self.init_conv = BaseConv(in_channels, out_channels, kernel_size, padding, stride)\n",
    "\n",
    "        self.down1 = DownConv(out_channels, 2 * out_channels, kernel_size,\n",
    "                              padding, stride)\n",
    "\n",
    "        self.down2 = DownConv(2 * out_channels, 4 * out_channels, kernel_size,\n",
    "                              padding, stride)\n",
    "\n",
    "        self.down3 = DownConv(4 * out_channels, 8 * out_channels, kernel_size,\n",
    "                              padding, stride)\n",
    "\n",
    "        self.up3 = UpConv(8 * out_channels, 4 * out_channels, 4 * out_channels,\n",
    "                          kernel_size, padding, stride)\n",
    "\n",
    "        self.up2 = UpConv(4 * out_channels, 2 * out_channels, 2 * out_channels,\n",
    "                          kernel_size, padding, stride)\n",
    "\n",
    "        self.up1 = UpConv(2 * out_channels, out_channels, out_channels,\n",
    "                          kernel_size, padding, stride)\n",
    "\n",
    "        self.out = nn.Conv2d(out_channels, 1, kernel_size, padding, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.init_conv(x)\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        # Decoder\n",
    "        x_up = self.up3(x3, x2)\n",
    "        x_up = self.up2(x_up, x1)\n",
    "        x_up = self.up1(x_up, x)\n",
    "        x_out = F.sigmoid(self.out(x_up))\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, visualize=False):\n",
    "    '''\n",
    "    Main training loop\n",
    "    '''\n",
    "    global win_loss\n",
    "    global win_images\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    # Iterate training set\n",
    "    for batch_idx, (data, mask) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data = data.cuda()\n",
    "            mask = mask.cuda()\n",
    "        data = Variable(data)\n",
    "        mask = Variable(mask.squeeze())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_mask = criterion(output.squeeze(), mask)\n",
    "        loss_dice = dice_loss(mask, output.squeeze())\n",
    "        loss = loss_mask + loss_dice\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % print_steps == 0:\n",
    "            loss_mask_data = loss_mask.data[0]\n",
    "            loss_dice_data = loss_dice.data[0]\n",
    "            train_losses.append(loss_mask_data)\n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLoss(dice): {:.6f}'.\n",
    "                format(epoch, batch_idx * len(data),\n",
    "                       len(train_loader.dataset), 100. * batch_idx / len(\n",
    "                           train_loader), loss_mask_data, loss_dice_data))\n",
    "            \n",
    "            x_idx = (epoch - 1) * len(train_loader) + batch_idx\n",
    "            losses = [loss_mask_data, loss_dice_data]\n",
    "            win_loss = visualize_losses(losses, x_idx, win_loss)\n",
    "            \n",
    "            if visualize:\n",
    "                # Visualize some images in Visdom\n",
    "                nb_images = 4\n",
    "                images_pred = output.data[:nb_images].cpu()\n",
    "                images_target = mask.data[:nb_images].cpu().unsqueeze(1)\n",
    "                images_input = data.data[:nb_images].cpu()\n",
    "                images = torch.zeros(3 * images_pred.size(0), *images_pred.size()[1:])\n",
    "                images[::3] = images_input\n",
    "                images[1::3] = images_pred\n",
    "                images[2::3] = images_target\n",
    "                # Resize images to fit in visdom\n",
    "                images = resize_tensors(images)\n",
    "                images = make_grid(images, nrow=3, pad_value=0.5)\n",
    "                win_images = visualize_images(\n",
    "                    images.numpy(), win_images, title='Training: input - prediction - target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    '''\n",
    "    Validation loop\n",
    "    '''\n",
    "    global win_eval_images\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    # Setup val_loss\n",
    "    val_mask_loss = 0\n",
    "    val_dice_loss = 0\n",
    "    # Disable gradients (to save memory)\n",
    "    with torch.no_grad():\n",
    "        # Iterate validation set\n",
    "        for data, mask in val_loader:\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "                mask = mask.cuda()\n",
    "            data = Variable(data)\n",
    "            mask = Variable(mask.squeeze())\n",
    "            output = model(data)\n",
    "            val_mask_loss += F.binary_cross_entropy(output.squeeze(), mask).data[0]\n",
    "            val_dice_loss += dice_loss(mask, output.squeeze()).data[0]\n",
    "    # Calculate mean of validation loss\n",
    "    val_mask_loss /= len(val_loader)\n",
    "    val_dice_loss /= len(val_loader)\n",
    "    val_losses.append(val_mask_loss)\n",
    "    print('Validation\\tLoss: {:.6f}\\tLoss(dice): {:.6f}'.format(val_mask_loss, val_dice_loss))\n",
    "    \n",
    "    # Visualize some images in Visdom\n",
    "    nb_images = 4\n",
    "    images_pred = output.data[:nb_images].cpu()\n",
    "    images_target = mask.data[:nb_images].cpu().unsqueeze(1)\n",
    "    images_input = data.data[:nb_images].cpu()\n",
    "    images = torch.zeros(3 * images_pred.size(0), *images_pred.size()[1:])\n",
    "    images[::3] = images_input\n",
    "    images[1::3] = images_pred\n",
    "    images[2::3] = images_target\n",
    "    # Resize images to fit in visdom\n",
    "    images = resize_tensors(images)\n",
    "    images = make_grid(images, nrow=3, pad_value=0.5)\n",
    "    win_eval_images = visualize_images(\n",
    "        images.numpy(), win_eval_images, title='Validation: input - prediction - target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get train data folders and split to training / validation set\n",
    "with open(data_paths, 'r') as f:\n",
    "    data_paths = f.readlines()\n",
    "image_paths = [line.split(',')[0].strip() for line in data_paths]\n",
    "target_paths = [line.split(',')[1].strip() for line in data_paths]\n",
    "\n",
    "# Split data into train/validation datasets\n",
    "im_path_train, im_path_val, tar_path_train, tar_path_val = split_data(\n",
    "    image_paths, target_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CellDataset(\n",
    "    image_paths=im_path_train,\n",
    "    target_paths=tar_path_train,\n",
    "    size=(96, 96),\n",
    "    train=True\n",
    ")\n",
    "val_dataset = CellDataset(\n",
    "    image_paths=im_path_val,\n",
    "    target_paths=tar_path_val,\n",
    "    size=(96, 96),\n",
    "    train=False\n",
    ")\n",
    "\n",
    "# Wrap in DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=12,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=12,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creae model\n",
    "model = ResUNet(\n",
    "    in_channels=1, out_channels=32, kernel_size=3, padding=1, stride=1)\n",
    "# Initialize weights\n",
    "model.apply(weights_init)\n",
    "# Push to GPU, if available\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create optimizer and scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "# Create criterion\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create visdom helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from visdom import Visdom\n",
    "from torchvision.utils import make_grid\n",
    "# Setup visdom\n",
    "viz = Visdom(port=6006)\n",
    "win_loss = None\n",
    "win_images = None\n",
    "win_eval_loss = None\n",
    "win_eval_images = None\n",
    "\n",
    "def visualize_losses(losses, x_idx, win):\n",
    "    if not win:\n",
    "        win = viz.line(\n",
    "            Y=np.column_stack(losses),\n",
    "            X=np.column_stack([x_idx] * len(losses)),\n",
    "            opts=dict(\n",
    "                showlegend=True,\n",
    "                xlabel='iteration',\n",
    "                ylabel='BCELoss',\n",
    "                ytype='log',\n",
    "                title='Losses',\n",
    "                legend=['Loss(mask)', 'Loss(dice)']))\n",
    "    else:\n",
    "        win = viz.line(\n",
    "            Y=np.column_stack(losses),\n",
    "            X=np.column_stack([x_idx] * len(losses)),\n",
    "            opts=dict(showlegend=True),\n",
    "            win=win,\n",
    "            update='append')\n",
    "    return win\n",
    "\n",
    "def visualize_images(images, win, title=''):\n",
    "    if not win:\n",
    "        win = viz.images(tensor=images, opts=dict(title=title))\n",
    "    else:\n",
    "        win = viz.images(tensor=images, win=win, opts=dict(title=title))\n",
    "    return win\n",
    "\n",
    "def resize_tensors(tensors, size=(128, 128)):\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    res = transforms.Resize(size=size)\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    images = torch.stack([to_tensor(res(to_pil(t))) for t in tensors])\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8640 (0%)]\tLoss: 0.702332\tLoss(dice): 0.535544\n",
      "Train Epoch: 1 [320/8640 (4%)]\tLoss: 0.699032\tLoss(dice): 0.567326\n",
      "Train Epoch: 1 [640/8640 (7%)]\tLoss: 0.692985\tLoss(dice): 0.569324\n",
      "Train Epoch: 1 [960/8640 (11%)]\tLoss: 0.687365\tLoss(dice): 0.555206\n",
      "Train Epoch: 1 [1280/8640 (15%)]\tLoss: 0.681835\tLoss(dice): 0.569749\n",
      "Train Epoch: 1 [1600/8640 (19%)]\tLoss: 0.674250\tLoss(dice): 0.595321\n",
      "Train Epoch: 1 [1920/8640 (22%)]\tLoss: 0.671852\tLoss(dice): 0.566566\n",
      "Train Epoch: 1 [2240/8640 (26%)]\tLoss: 0.669534\tLoss(dice): 0.546438\n",
      "Train Epoch: 1 [2560/8640 (30%)]\tLoss: 0.664101\tLoss(dice): 0.553792\n",
      "Train Epoch: 1 [2880/8640 (33%)]\tLoss: 0.660521\tLoss(dice): 0.546547\n",
      "Train Epoch: 1 [3200/8640 (37%)]\tLoss: 0.654070\tLoss(dice): 0.561910\n",
      "Train Epoch: 1 [3520/8640 (41%)]\tLoss: 0.646566\tLoss(dice): 0.574051\n",
      "Train Epoch: 1 [3840/8640 (44%)]\tLoss: 0.634483\tLoss(dice): 0.614093\n",
      "Train Epoch: 1 [4160/8640 (48%)]\tLoss: 0.633179\tLoss(dice): 0.583968\n",
      "Train Epoch: 1 [4480/8640 (52%)]\tLoss: 0.632120\tLoss(dice): 0.561997\n",
      "Train Epoch: 1 [4800/8640 (56%)]\tLoss: 0.620613\tLoss(dice): 0.588261\n",
      "Train Epoch: 1 [5120/8640 (59%)]\tLoss: 0.620461\tLoss(dice): 0.572780\n",
      "Train Epoch: 1 [5440/8640 (63%)]\tLoss: 0.613427\tLoss(dice): 0.572841\n",
      "Train Epoch: 1 [5760/8640 (67%)]\tLoss: 0.615153\tLoss(dice): 0.552154\n",
      "Train Epoch: 1 [6080/8640 (70%)]\tLoss: 0.610388\tLoss(dice): 0.546827\n",
      "Train Epoch: 1 [6400/8640 (74%)]\tLoss: 0.590131\tLoss(dice): 0.591365\n",
      "Train Epoch: 1 [6720/8640 (78%)]\tLoss: 0.594355\tLoss(dice): 0.552597\n",
      "Train Epoch: 1 [7040/8640 (81%)]\tLoss: 0.588499\tLoss(dice): 0.558860\n",
      "Train Epoch: 1 [7360/8640 (85%)]\tLoss: 0.581176\tLoss(dice): 0.549652\n",
      "Train Epoch: 1 [7680/8640 (89%)]\tLoss: 0.574865\tLoss(dice): 0.542013\n",
      "Train Epoch: 1 [8000/8640 (93%)]\tLoss: 0.579432\tLoss(dice): 0.507733\n",
      "Train Epoch: 1 [8320/8640 (96%)]\tLoss: 0.564734\tLoss(dice): 0.522730\n",
      "Validation\tLoss: 0.551707\tLoss(dice): 0.527650\n",
      "Train Epoch: 2 [0/8640 (0%)]\tLoss: 0.538426\tLoss(dice): 0.566941\n",
      "Train Epoch: 2 [320/8640 (4%)]\tLoss: 0.545332\tLoss(dice): 0.514550\n",
      "Train Epoch: 2 [640/8640 (7%)]\tLoss: 0.527982\tLoss(dice): 0.529266\n",
      "Train Epoch: 2 [960/8640 (11%)]\tLoss: 0.518437\tLoss(dice): 0.514439\n",
      "Train Epoch: 2 [1280/8640 (15%)]\tLoss: 0.506704\tLoss(dice): 0.513972\n",
      "Train Epoch: 2 [1600/8640 (19%)]\tLoss: 0.500289\tLoss(dice): 0.496147\n",
      "Train Epoch: 2 [1920/8640 (22%)]\tLoss: 0.492971\tLoss(dice): 0.475825\n",
      "Train Epoch: 2 [2240/8640 (26%)]\tLoss: 0.480202\tLoss(dice): 0.458985\n",
      "Train Epoch: 2 [2560/8640 (30%)]\tLoss: 0.448906\tLoss(dice): 0.485557\n",
      "Train Epoch: 2 [2880/8640 (33%)]\tLoss: 0.455987\tLoss(dice): 0.429715\n",
      "Train Epoch: 2 [3200/8640 (37%)]\tLoss: 0.431426\tLoss(dice): 0.426724\n",
      "Train Epoch: 2 [3520/8640 (41%)]\tLoss: 0.414436\tLoss(dice): 0.427541\n",
      "Train Epoch: 2 [3840/8640 (44%)]\tLoss: 0.404342\tLoss(dice): 0.397486\n",
      "Train Epoch: 2 [4160/8640 (48%)]\tLoss: 0.386150\tLoss(dice): 0.398418\n",
      "Train Epoch: 2 [4480/8640 (52%)]\tLoss: 0.356288\tLoss(dice): 0.397147\n",
      "Train Epoch: 2 [4800/8640 (56%)]\tLoss: 0.341743\tLoss(dice): 0.382674\n",
      "Train Epoch: 2 [5120/8640 (59%)]\tLoss: 0.341323\tLoss(dice): 0.344301\n",
      "Train Epoch: 2 [5440/8640 (63%)]\tLoss: 0.331125\tLoss(dice): 0.318080\n",
      "Train Epoch: 2 [5760/8640 (67%)]\tLoss: 0.295948\tLoss(dice): 0.334394\n",
      "Train Epoch: 2 [6080/8640 (70%)]\tLoss: 0.300371\tLoss(dice): 0.305055\n",
      "Train Epoch: 2 [6400/8640 (74%)]\tLoss: 0.290541\tLoss(dice): 0.274897\n",
      "Train Epoch: 2 [6720/8640 (78%)]\tLoss: 0.272099\tLoss(dice): 0.266478\n",
      "Train Epoch: 2 [7040/8640 (81%)]\tLoss: 0.241712\tLoss(dice): 0.290770\n",
      "Train Epoch: 2 [7360/8640 (85%)]\tLoss: 0.239624\tLoss(dice): 0.256948\n",
      "Train Epoch: 2 [7680/8640 (89%)]\tLoss: 0.221741\tLoss(dice): 0.247108\n",
      "Train Epoch: 2 [8000/8640 (93%)]\tLoss: 0.210717\tLoss(dice): 0.239993\n",
      "Train Epoch: 2 [8320/8640 (96%)]\tLoss: 0.210211\tLoss(dice): 0.225008\n",
      "Validation\tLoss: 0.201630\tLoss(dice): 0.218621\n",
      "Train Epoch: 3 [0/8640 (0%)]\tLoss: 0.207300\tLoss(dice): 0.219218\n",
      "Train Epoch: 3 [320/8640 (4%)]\tLoss: 0.203545\tLoss(dice): 0.207316\n",
      "Train Epoch: 3 [640/8640 (7%)]\tLoss: 0.175600\tLoss(dice): 0.204866\n",
      "Train Epoch: 3 [960/8640 (11%)]\tLoss: 0.180190\tLoss(dice): 0.198479\n",
      "Train Epoch: 3 [1280/8640 (15%)]\tLoss: 0.179179\tLoss(dice): 0.194352\n",
      "Train Epoch: 3 [1600/8640 (19%)]\tLoss: 0.188381\tLoss(dice): 0.166109\n",
      "Train Epoch: 3 [1920/8640 (22%)]\tLoss: 0.158078\tLoss(dice): 0.185017\n",
      "Train Epoch: 3 [2240/8640 (26%)]\tLoss: 0.156972\tLoss(dice): 0.168872\n",
      "Train Epoch: 3 [2560/8640 (30%)]\tLoss: 0.174340\tLoss(dice): 0.157755\n",
      "Train Epoch: 3 [2880/8640 (33%)]\tLoss: 0.155718\tLoss(dice): 0.167782\n",
      "Train Epoch: 3 [3200/8640 (37%)]\tLoss: 0.156057\tLoss(dice): 0.153468\n",
      "Train Epoch: 3 [3520/8640 (41%)]\tLoss: 0.151440\tLoss(dice): 0.149390\n",
      "Train Epoch: 3 [3840/8640 (44%)]\tLoss: 0.152034\tLoss(dice): 0.144427\n",
      "Train Epoch: 3 [4160/8640 (48%)]\tLoss: 0.143270\tLoss(dice): 0.147711\n",
      "Train Epoch: 3 [4480/8640 (52%)]\tLoss: 0.156614\tLoss(dice): 0.142038\n",
      "Train Epoch: 3 [4800/8640 (56%)]\tLoss: 0.142328\tLoss(dice): 0.141619\n",
      "Train Epoch: 3 [5120/8640 (59%)]\tLoss: 0.150387\tLoss(dice): 0.142512\n",
      "Train Epoch: 3 [5440/8640 (63%)]\tLoss: 0.158454\tLoss(dice): 0.122958\n",
      "Train Epoch: 3 [5760/8640 (67%)]\tLoss: 0.158536\tLoss(dice): 0.129466\n",
      "Train Epoch: 3 [6080/8640 (70%)]\tLoss: 0.132136\tLoss(dice): 0.139165\n",
      "Train Epoch: 3 [6400/8640 (74%)]\tLoss: 0.128411\tLoss(dice): 0.142007\n",
      "Train Epoch: 3 [6720/8640 (78%)]\tLoss: 0.143975\tLoss(dice): 0.123854\n",
      "Train Epoch: 3 [7040/8640 (81%)]\tLoss: 0.135616\tLoss(dice): 0.130716\n",
      "Train Epoch: 3 [7360/8640 (85%)]\tLoss: 0.132488\tLoss(dice): 0.128208\n",
      "Train Epoch: 3 [7680/8640 (89%)]\tLoss: 0.160774\tLoss(dice): 0.125977\n",
      "Train Epoch: 3 [8000/8640 (93%)]\tLoss: 0.119456\tLoss(dice): 0.125293\n",
      "Train Epoch: 3 [8320/8640 (96%)]\tLoss: 0.141581\tLoss(dice): 0.120338\n",
      "Validation\tLoss: 0.131993\tLoss(dice): 0.121501\n",
      "Train Epoch: 4 [0/8640 (0%)]\tLoss: 0.131685\tLoss(dice): 0.129929\n",
      "Train Epoch: 4 [320/8640 (4%)]\tLoss: 0.111093\tLoss(dice): 0.130584\n",
      "Train Epoch: 4 [640/8640 (7%)]\tLoss: 0.151993\tLoss(dice): 0.117368\n",
      "Train Epoch: 4 [960/8640 (11%)]\tLoss: 0.121918\tLoss(dice): 0.120243\n",
      "Train Epoch: 4 [1280/8640 (15%)]\tLoss: 0.114173\tLoss(dice): 0.118927\n",
      "Train Epoch: 4 [1600/8640 (19%)]\tLoss: 0.137680\tLoss(dice): 0.111153\n",
      "Train Epoch: 4 [1920/8640 (22%)]\tLoss: 0.127415\tLoss(dice): 0.122961\n",
      "Train Epoch: 4 [2240/8640 (26%)]\tLoss: 0.125162\tLoss(dice): 0.107869\n",
      "Train Epoch: 4 [2560/8640 (30%)]\tLoss: 0.101919\tLoss(dice): 0.118129\n",
      "Train Epoch: 4 [2880/8640 (33%)]\tLoss: 0.126712\tLoss(dice): 0.106373\n",
      "Train Epoch: 4 [3200/8640 (37%)]\tLoss: 0.133675\tLoss(dice): 0.113163\n",
      "Train Epoch: 4 [3520/8640 (41%)]\tLoss: 0.128302\tLoss(dice): 0.106352\n",
      "Train Epoch: 4 [3840/8640 (44%)]\tLoss: 0.146014\tLoss(dice): 0.109040\n",
      "Train Epoch: 4 [4160/8640 (48%)]\tLoss: 0.131383\tLoss(dice): 0.115028\n",
      "Train Epoch: 4 [4480/8640 (52%)]\tLoss: 0.129233\tLoss(dice): 0.113924\n",
      "Train Epoch: 4 [4800/8640 (56%)]\tLoss: 0.131074\tLoss(dice): 0.108867\n",
      "Train Epoch: 4 [5120/8640 (59%)]\tLoss: 0.135127\tLoss(dice): 0.110741\n",
      "Train Epoch: 4 [5440/8640 (63%)]\tLoss: 0.126893\tLoss(dice): 0.101287\n",
      "Train Epoch: 4 [5760/8640 (67%)]\tLoss: 0.129308\tLoss(dice): 0.107607\n",
      "Train Epoch: 4 [6080/8640 (70%)]\tLoss: 0.132713\tLoss(dice): 0.106569\n",
      "Train Epoch: 4 [6400/8640 (74%)]\tLoss: 0.124632\tLoss(dice): 0.104665\n",
      "Train Epoch: 4 [6720/8640 (78%)]\tLoss: 0.156434\tLoss(dice): 0.104674\n",
      "Train Epoch: 4 [7040/8640 (81%)]\tLoss: 0.136559\tLoss(dice): 0.102366\n",
      "Train Epoch: 4 [7360/8640 (85%)]\tLoss: 0.121222\tLoss(dice): 0.098888\n",
      "Train Epoch: 4 [7680/8640 (89%)]\tLoss: 0.122952\tLoss(dice): 0.105097\n",
      "Train Epoch: 4 [8000/8640 (93%)]\tLoss: 0.107970\tLoss(dice): 0.108805\n",
      "Train Epoch: 4 [8320/8640 (96%)]\tLoss: 0.111281\tLoss(dice): 0.107355\n",
      "Validation\tLoss: 0.124113\tLoss(dice): 0.102351\n",
      "Train Epoch: 5 [0/8640 (0%)]\tLoss: 0.115042\tLoss(dice): 0.102073\n",
      "Train Epoch: 5 [320/8640 (4%)]\tLoss: 0.119878\tLoss(dice): 0.099258\n",
      "Train Epoch: 5 [640/8640 (7%)]\tLoss: 0.129630\tLoss(dice): 0.097619\n",
      "Train Epoch: 5 [960/8640 (11%)]\tLoss: 0.121770\tLoss(dice): 0.101719\n",
      "Train Epoch: 5 [1280/8640 (15%)]\tLoss: 0.140221\tLoss(dice): 0.094950\n",
      "Train Epoch: 5 [1600/8640 (19%)]\tLoss: 0.133804\tLoss(dice): 0.097828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-103:\n",
      "Process Process-102:\n",
      "Process Process-97:\n",
      "Process Process-99:\n",
      "Process Process-105:\n",
      "Process Process-108:\n",
      "Process Process-107:\n",
      "Process Process-98:\n",
      "Process Process-106:\n",
      "Process Process-100:\n",
      "Process Process-104:\n",
      "Process Process-101:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "    r = index_queue.get()\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    r = index_queue.get()\n",
      "    r = index_queue.get()\n",
      "    r = index_queue.get()\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "Traceback (most recent call last):\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    self.run()\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    r = index_queue.get()\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    r = index_queue.get()\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    r = index_queue.get()\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "  File \"<ipython-input-5-c2e28ed8d126>\", line 48, in __getitem__\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    r = index_queue.get()\n",
      "    racquire()\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    racquire()\n",
      "    x, y = self.transform(image, mask)\n",
      "    racquire()\n",
      "    racquire()\n",
      "    racquire()\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-c2e28ed8d126>\", line 15, in transform\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "    return recv()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "    r = index_queue.get()\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pbialecki/libs/pytorch/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    image = self.resize_image(image)\n",
      "    buf = self.recv_bytes()\n",
      "  File \"build/bdist.linux-x86_64/egg/torchvision/transforms/transforms.py\", line 147, in __call__\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "    return F.resize(img, self.size, self.interpolation)\n",
      "  File \"build/bdist.linux-x86_64/egg/torchvision/transforms/functional.py\", line 199, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "  File \"/home/pbialecki/anaconda2/lib/python2.7/site-packages/PIL/Image.py\", line 1747, in resize\n",
      "    return self._new(self.im.resize(size, resample, box))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [1920/8640 (22%)]\tLoss: 0.132754\tLoss(dice): 0.101808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception socket.error: error(2, 'No such file or directory') in <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7f15411e4b10>> ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c064e38148e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5c5b2a978d0d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, visualize)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss_dice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_mask\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_dice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pbialecki/libs/pytorch/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pbialecki/libs/pytorch/torch/nn/modules/loss.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    433\u001b[0m         return F.binary_cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m    434\u001b[0m                                       \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                                       reduce=self.reduce)\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pbialecki/libs/pytorch/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce)\u001b[0m\n\u001b[1;32m   1436\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "train_losses, val_losses = [], []\n",
    "epochs = 30\n",
    "for epoch in range(1, epochs):\n",
    "    train(epoch, visualize=True)\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
